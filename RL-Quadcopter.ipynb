{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Train a Quadcopter How to Fly\n",
    "\n",
    "Design an agent that can fly a quadcopter, and then train it using a reinforcement learning algorithm of your choice! Try to apply the techniques you have learnt, but also feel free to come up with innovative ideas and test them.\n",
    "\n",
    "![Quadcopter doing a flip trying to takeoff from the ground](images/quadcopter_tumble.png)\n",
    "\n",
    "## Instructions\n",
    "\n",
    "> **Note**: If you haven't done so already, follow the steps in this repo's README to install ROS, and ensure that the simulator is running and correctly connecting to ROS.\n",
    "\n",
    "When you are ready to start coding, take a look at the `quad_controller_rl/src/` (source) directory to better understand the structure. Here are some of the salient items:\n",
    "\n",
    "- `src/`: Contains all the source code for the project.\n",
    "  - `quad_controller_rl/`: This is the root of the Python package you'll be working in.\n",
    "  - ...\n",
    "  - `tasks/`: Define your tasks (environments) in this sub-directory.\n",
    "    - `__init__.py`: When you define a new task, you'll have to import it here.\n",
    "    - `base_task.py`: Generic base class for all tasks, with documentation.\n",
    "    - `takeoff.py`: This is the first task, already defined for you, and set to run by default.\n",
    "  - ...\n",
    "  - `agents/`: Develop your reinforcement learning agents here.\n",
    "    - `__init__.py`: When you define a new agent, you'll have to import it here, just like tasks.\n",
    "    - `base_agent.py`: Generic base class for all agents, with documentation.\n",
    "    - `policy_search.py`: A sample agent has been provided here, and is set to run by default.\n",
    "  - ...\n",
    "\n",
    "### Tasks\n",
    "\n",
    "Open up the base class for tasks, `BaseTask`, defined in `tasks/base_task.py`:\n",
    "\n",
    "```python\n",
    "class BaseTask:\n",
    "    \"\"\"Generic base class for reinforcement learning tasks.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Define state and action spaces, initialize other task parameters.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def set_agent(self, agent):\n",
    "        \"\"\"Set an agent to carry out this task; to be called from update.\"\"\"\n",
    "        self.agent = agent\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset task and return initial condition.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update(self, timestamp, pose, angular_velocity, linear_acceleration):\n",
    "        \"\"\"Process current data, call agent, return action and done flag.\"\"\"\n",
    "        raise NotImplementedError            \n",
    "```\n",
    "\n",
    "All tasks must inherit from this class to function properly. You will need to override the `reset()` and `update()` methods when defining a task, otherwise you will get `NotImplementedError`'s. Besides these two, you should define the state (observation) space and the action space for the task in the constructor, `__init__()`, and initialize any other variables you may need to run the task.\n",
    "\n",
    "Now compare this with the first concrete task `Takeoff`, defined in `tasks/takeoff.py`:\n",
    "\n",
    "```python\n",
    "class Takeoff(BaseTask):\n",
    "    \"\"\"Simple task where the goal is to lift off the ground and reach a target height.\"\"\"\n",
    "    ...\n",
    "```\n",
    "\n",
    "In `__init__()`, notice how the state and action spaces are defined using [OpenAI Gym spaces](https://gym.openai.com/docs/#spaces), like [`Box`](https://github.com/openai/gym/blob/master/gym/spaces/box.py). These objects provide a clean and powerful interface for agents to explore. For instance, they can inspect the dimensionality of a space (`shape`), ask for the limits (`high` and `low`), or even sample a bunch of observations using the `sample()` method, before beginning to interact with the environment. We also set a time limit (`max_duration`) for each episode here, and the height (`target_z`) that the quadcopter needs to reach for a successful takeoff.\n",
    "\n",
    "The `reset()` method is meant to give you a chance to reset/initialize any variables you need in order to prepare for the next episode. You do not need to call it yourself; it will be invoked externally. And yes, it will be called once before each episode, including the very first one. Here `Takeoff` doesn't have any episode variables to initialize, but it must return a valid _initial condition_ for the task, which is a tuple consisting of a [`Pose`](http://docs.ros.org/api/geometry_msgs/html/msg/Pose.html) and [`Twist`](http://docs.ros.org/api/geometry_msgs/html/msg/Twist.html) object. These are ROS message types used to convey the pose (position, orientation) and velocity (linear, angular) you want the quadcopter to have at the beginning of an episode. You may choose to supply the same initial values every time, or change it a little bit, e.g. `Takeoff` drops the quadcopter off from a small height with a bit of randomness.\n",
    "\n",
    "> **Tip**: Slightly randomized initial conditions can help the agent explore the state space faster.\n",
    "\n",
    "Finally, the `update()` method is perhaps the most important. This is where you define the dynamics of the task and engage the agent. It is called by a ROS process periodically (roughly 30 times a second, by default), with current data from the simulation. A number of arguments are available: `timestamp` (you can use this to check for timeout, or compute velocities), `pose` (position, orientation of the quadcopter), `angular_velocity`, and `linear_acceleration`. You do not have to include all these variables in every task, e.g. `Takeoff` only uses pose information, and even that requires a 7-element state vector.\n",
    "\n",
    "Once you have prepared the state you want to pass on to your agent, you will need to compute the reward, and check whether the episode is complete (e.g. agent crossed the time limit, or reached a certain height). Note that these two things (`reward` and `done`) are based on actions that the agent took in the past. When you are writing your own agents, you have to be mindful of this.\n",
    "\n",
    "Now you can pass in the `state`, `reward` and `done` values to the agent's `step()` method and expect an action vector back that matches the action space that you have defined, in this case a `Box(6,)`. After checking that the action vector is non-empty, and clamping it to the space limits, you have to convert it into a ROS `Wrench` message. The first 3 elements of the action vector are interpreted as force in x, y, z directions, and the remaining 3 elements convey the torque to be applied around those axes, respectively.\n",
    "\n",
    "Return the `Wrench` object (or `None` if you don't want to take any action) and the `done` flag from your `update()` method (note that when `done` is `True`, the `Wrench` object is ignored, so you can return `None` instead). This will be passed back to the simulation as a control command, and will affect the quadcopter's pose, orientation, velocity, etc. You will be able to gauge the effect when the `update()` method is called in the next time step.\n",
    "\n",
    "### Agents\n",
    "\n",
    "Reinforcement learning agents are defined in a similar way. Open up the generic agent class, `BaseAgent`, defined in `agents/base_agent.py`, and the sample agent `RandomPolicySearch` defined in `agents/policy_search.py`. They are actually even simpler to define - you only need to implement the `step()` method that is discussed above. It needs to consume `state` (vector), `reward` (scalar value) and `done` (boolean), and produce an `action` (vector). The state and action vectors must match the respective space indicated by the task. And that's it!\n",
    "\n",
    "Well, that's just to get things working correctly! The sample agent given `RandomPolicySearch` uses a very simplistic linear policy to directly compute the action vector as a dot product of the state vector and a matrix of weights. Then, it randomly perturbs the parameters by adding some Gaussian noise, to produce a different policy. Based on the average reward obtained in each episode (\"score\"), it keeps track of the best set of parameters found so far, how the score is changing, and accordingly tweaks a scaling factor to widen or tighten the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width: 100%; text-align: center;\">\n",
       "    <h3>Teach a Quadcopter How to Tumble</h3>\n",
       "    <video poster=\"images/quadcopter_tumble.png\" width=\"640\" controls muted>\n",
       "        <source src=\"images/quadcopter_tumble.mp4\" type=\"video/mp4\" />\n",
       "        <p>Video: Quadcopter tumbling, trying to get off the ground</p>\n",
       "    </video>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div style=\"width: 100%; text-align: center;\">\n",
    "    <h3>Teach a Quadcopter How to Tumble</h3>\n",
    "    <video poster=\"images/quadcopter_tumble.png\" width=\"640\" controls muted>\n",
    "        <source src=\"images/quadcopter_tumble.mp4\" type=\"video/mp4\" />\n",
    "        <p>Video: Quadcopter tumbling, trying to get off the ground</p>\n",
    "    </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this agent performs very poorly on the task. It does manage to move the quadcopter, which is good, but instead of a stable takeoff, it often leads to dizzying cartwheels and somersaults! And that's where you come in - your first _task_ is to design a better agent for this takeoff task. Instead of messing with the sample agent, create new file in the `agents/` directory, say `policy_gradients.py`, and define your own agent in it. Remember to inherit from the base agent class, e.g.:\n",
    "\n",
    "```python\n",
    "class DDPG(BaseAgent):\n",
    "    ...\n",
    "```\n",
    "\n",
    "You can borrow whatever you need from the sample agent, including ideas on how you might modularize your code (using helper methods like `act()`, `learn()`, `reset_episode_vars()`, etc.).\n",
    "\n",
    "> **Note**: This setup may look similar to the common OpenAI Gym paradigm, but there is one small yet important difference. Instead of the agent calling a method on the environment (to execute an action and obtain the resulting state, reward and done value), here it is the task that is calling a method on the agent (`step()`). If you plan to store experience tuples for learning, you will need to cache the last state ($S_{t-1}$) and last action taken ($A_{t-1}$), then in the next time step when you get the new state ($S_t$) and reward ($R_t$), you can store them along with the `done` flag ($\\left\\langle S_{t-1}, A_{t-1}, R_t, S_t, \\mathrm{done?}\\right\\rangle$).\n",
    "\n",
    "When an episode ends, the agent receives one last call to the `step()` method with `done` set to `True` - this is your chance to perform any cleanup/reset/batch-learning (note that no reset method is called on an agent externally). The action returned on this last call is ignored, so you may safely return `None`. The next call would be the beginning of a new episode.\n",
    "\n",
    "One last thing - in order to run your agent, you will have to edit `agents/__init__.py` and import your agent class in it, e.g.:\n",
    "\n",
    "```python\n",
    "from quad_controller_rl.agents.policy_gradients import DDPG\n",
    "```\n",
    "\n",
    "Then, while launching ROS, you will need to specify this class name on the commandline/terminal:\n",
    "\n",
    "```bash\n",
    "roslaunch quad_controller_rl rl_controller.launch agent:=DDPG\n",
    "```\n",
    "\n",
    "Okay, now the first task is cut out for you - follow the instructions below to implement an agent that learns to take off from the ground. For the remaining tasks, you get to define the tasks as well as the agents! Use the `Takeoff` task as a guide, and refer to the `BaseTask` docstrings for the different methods you need to override. Use some debug print statements to understand the flow of control better. And just like creating new agents, new tasks must inherit `BaseTask`, they need be imported into `tasks/__init__.py`, and specified on the commandline when running:\n",
    "\n",
    "```bash\n",
    "roslaunch quad_controller_rl rl_controller.launch task:=Hover agent:=DDPG\n",
    "```\n",
    "\n",
    "> **Tip**: You typically need to launch ROS and then run the simulator manually. But you can automate that process by either copying/symlinking your simulator to `quad_controller_rl/sim/DroneSim` (`DroneSim` must be an executable/link to one), or by specifying it on the command line, as follows:\n",
    "> \n",
    "> ```bash\n",
    "> roslaunch quad_controller_rl rl_controller.launch task:=Hover agent:=DDPG sim:=<full path>\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Takeoff\n",
    "\n",
    "### Implement takeoff agent\n",
    "\n",
    "Train an agent to successfully lift off from the ground and reach a certain threshold height. Develop your agent in a file under `agents/` as described above, implementing at least the `step()` method, and any other supporting methods that might be necessary. You may use any reinforcement learning algorithm of your choice (note that the action space consists of continuous variables, so that may somewhat limit your choices).\n",
    "\n",
    "The task has already been defined (in `tasks/takeoff.py`), which you should not edit. The default target height (Z-axis value) to reach is 10 units above the ground. And the reward function is essentially the negative absolute distance from that set point (upto some threshold). An episode ends when the quadcopter reaches the target height (x and y values, orientation, velocity, etc. are ignored), or when the maximum duration is crossed (5 seconds).  See `Takeoff.update()` for more details, including episode bonus/penalty.\n",
    "\n",
    "As you develop your agent, it's important to keep an eye on how it's performing. Build in a mechanism to log/save the total rewards obtained in each episode to file. Once you are satisfied with your agent's performance, return to this notebook to plot episode rewards, and answer the questions below.\n",
    "\n",
    "### Plot episode rewards\n",
    "\n",
    "Plot the total rewards obtained in each episode, either from a single run, or averaged over multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from quad_controller_rl import util\n",
    "def plot_results(file):\n",
    "    df_stats = pd.read_csv('/home/robond/catws/src/RL-Quadcopter/quad_controller_rl/out/' + file)\n",
    "    df_stats[['total_reward']].plot(title=\"Episode Rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f13a0599908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Read and plot episode rewards\n",
    "plot_results('takeoff.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What algorithm did you use? Briefly discuss why you chose it for this task.\n",
    "\n",
    "**A**: I have used DDPG algorithm which seems to work greart for continuos action space problems. All the individual tasks of this project exhibit continuous action space, which require an algorithm that can work in this situation. Also, it is a model free algorithm, which eliminates the dimensionality issue. The off policy nature of this algorithm \n",
    "\n",
    "\n",
    "**Q**: Using the episode rewards plot, discuss how the agent learned over time.\n",
    "\n",
    "- Was it an easy task to learn or hard?\n",
    "- Was there a gradual learning curve, or an aha moment?\n",
    "- How good was the final performance of the agent? (e.g. mean rewards over the last 10 episodes)\n",
    "\n",
    "**A**:It appears to me that takeoff was rather easy, as the rewards graph came out to be a flat line, indicating that it has learned right from the beginning! However, I am skeptical if it has learned anything really. Although, when I run the simulation in DroneSim, it consistently takes off (video clipping posted below). One reason that I can think could be responsible for such easy learning is that the constraint placed on z direction for takeoff reduced the number of degrees of freedom to a minimum, which was not difficult to learn using NN. The final performance appears to be very good, the rewards stayed at the maximum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width: 100%; text-align: center;\">\n",
       "    <h3>Teach a Quadcopter How to Takeoff</h3>\n",
       "    <video poster=\"images/takeoff.png\" width=\"640\" controls muted>\n",
       "        <source src=\"images/takeoff.mp4\" type=\"video/mp4\" />\n",
       "        <p>Video: Quadcopter taking off the ground</p>\n",
       "    </video>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div style=\"width: 100%; text-align: center;\">\n",
    "    <h3>Teach a Quadcopter How to Takeoff</h3>\n",
    "    <video poster=\"images/takeoff.png\" width=\"640\" controls muted>\n",
    "        <source src=\"images/takeoff.mp4\" type=\"video/mp4\" />\n",
    "        <p>Video: Quadcopter taking off the ground</p>\n",
    "    </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Hover\n",
    "\n",
    "### Implement hover agent\n",
    "\n",
    "Now, your agent must take off and hover at the specified set point (say, 10 units above the ground). Same as before, you will need to create an agent and implement the `step()` method (and any other supporting methods) to apply your reinforcement learning algorithm. You may use the same agent as before, if you think your implementation is robust, and try to train it on the new task. But then remember to store your previous model weights/parameters, in case your results were worth keeping.\n",
    "\n",
    "### States and rewards\n",
    "\n",
    "Even if you can use the same agent, you will need to create a new task, which will allow you to change the state representation you pass in, how you verify when the episode has ended (the quadcopter needs to hover for at least a few seconds), etc. In this hover task, you may want to pass in the target height as part of the state (otherwise how would the agent know where you want it to go?). You may also need to revisit how rewards are computed. You can do all this in a new task file, e.g. `tasks/hover.py` (remember to follow the steps outlined above to create a new task):\n",
    "\n",
    "```python\n",
    "class Hover(BaseTask):\n",
    "    ...\n",
    "```\n",
    "\n",
    "**Q**: Did you change the state representation or reward function? If so, please explain below what worked best for you, and why you chose that scheme. Include short code snippet(s) if needed.\n",
    "\n",
    "**A**: Technically, all the three tasks involved only movement along z direction only. So, for hovering also, I evaluated the height in z direction at the end of each episode to see if it is able to maintain its hovering height without drifting around in x and y direction. \n",
    "\n",
    "Here's code for calculating reward for hovering:\n",
    "\n",
    "    def calculate_reward(self, timestamp, pose):\n",
    "        done = False\n",
    "        reward = 0\n",
    "        if pose.position.z > self.target_z:\n",
    "            reward += 1.0  # bonus reward\n",
    "            done = True\n",
    "        elif timestamp > self.max_duration:  # agent has run out of time\n",
    "            reward -= 0.95  # extra penalty\n",
    "            done = True\n",
    "        return reward, done\n",
    "\n",
    "### Implementation notes\n",
    "\n",
    "**Q**: Discuss your implementation below briefly, using the following questions as a guide:\n",
    "\n",
    "- What algorithm(s) did you try? What worked best for you?\n",
    "- What was your final choice of hyperparameters (such as $\\alpha$, $\\gamma$, $\\epsilon$, etc.)?\n",
    "- What neural network architecture did you use (if any)? Specify layers, sizes, activation functions, etc.\n",
    "\n",
    "**A**: So far, I have tried DDPG for all the three tasks. I plan to try A3C also.  As for implementation, I have used the shell code provided for actor, critic, and ddpg agent (and modified to make it work). The OUNoise was implemented after having a look at the following link:\n",
    "https://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "\n",
    "The noise parameters are: $\\mu$ = 1.2, $\\sigma$ = 0.3, $\\dt$ = 0.001, $\\theta$ = 1.\n",
    "\n",
    "NN for actor: 3 layers of dense (sizes: 16, 32, 16) with ReLU activation, followed by the final dense layer with sigmoid activation. I have used a learning rate of 0.0001 for actor.\n",
    "NN for critic: Two strands for state and action, with each having two dense layers (sizes: 8, 16) with ReLU activation. The state and action networks are combined finally to form a single output layer with ReLU activation. This layer is connected to a dense layer with the output Q values. I have used a learning rate of 0.001.\n",
    "\n",
    "\n",
    "### Plot episode rewards\n",
    "\n",
    "As before, plot the episode rewards, either from a single run, or averaged over multiple runs. Comment on any changes in learning behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f139d9b3a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Read and plot episode rewards\n",
    "plot_results('hover.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Landing\n",
    "\n",
    "What goes up, must come down! But safely!\n",
    "\n",
    "### Implement landing agent\n",
    "\n",
    "This time, you will need to edit the starting state of the quadcopter to place it at a position above the ground (at least 10 units). And change the reward function to make the agent learn to settle down _gently_. Again, create a new task for this (e.g. `Landing` in `tasks/landing.py`), and implement the changes. Note that you will have to modify the `reset()` method to return a position in the air, perhaps with some upward velocity to mimic a recent takeoff.\n",
    "\n",
    "Once you're satisfied with your task definition, create another agent or repurpose an existing one to learn this task. This might be a good chance to try out a different approach or algorithm.\n",
    "\n",
    "### Initial condition, states and rewards\n",
    "\n",
    "**Q**: How did you change the initial condition (starting state), state representation and/or reward function? Please explain below what worked best for you, and why you chose that scheme. Were you able to build in a reward mechanism for landing gently?\n",
    "\n",
    "**A**: Similar to take off task, I constrained the state and action spaces to z direction only for the landing task. Since the state and action spaces were constrained in the z direction only, reward computation involved only this directionality. The code for reward compuation is given below:\n",
    "\n",
    "    def calculate_reward(self, timestamp, pose, velocity, linear_acceleration):\n",
    "        # Compute reward / penalty and check if this episode is complete\n",
    "        done = False\n",
    "\n",
    "        reward = 0\n",
    "        if pose.position.z <= self.target_z:  # agent has landed\n",
    "            reward += 1.0  # bonus reward\n",
    "            done = True\n",
    "        elif timestamp > self.max_duration:  # agent has run out of time\n",
    "            reward -= 0.9  # extra penalty\n",
    "            done = True\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "The above reward mechanism didn't create a very gentle landing situation and for what I have tried additional rewards like the speed in z direction reaching 0 upon descending to the landing elevation etc., but that didn't work out too well. So, in the final code, I left the reward function tied to z position only.\n",
    "\n",
    "### Implementation notes\n",
    "\n",
    "**Q**: Discuss your implementation below briefly, using the same questions as before to guide you.\n",
    "\n",
    "**A**: I used the same DDPG algorithm, that was used for take off. The actor- critic NN were also used same. Since my eventual goal was to use a single critic- actor NN model for all the three tasks, I have reused the NN model developed for take off task above.\n",
    "\n",
    "### Plot episode rewards\n",
    "\n",
    "As before, plot the episode rewards, either from a single run, or averaged over multiple runs. This task is a little different from the previous ones, since you're starting in the air. Was it harder to learn? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f139d830b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Read and plot episode rewards\n",
    "plot_results('landing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Combined\n",
    "\n",
    "In order to design a complete flying system, you will need to incorporate all these basic behaviors into a single agent.\n",
    "\n",
    "### Setup end-to-end task\n",
    "\n",
    "The end-to-end task we are considering here is simply to takeoff, hover in-place for some duration, and then land. Time to create another task! But think about how you might go about it. Should it be one meta-task that activates appropriate sub-tasks, one at a time? Or would a single combined task with something like waypoints be easier to implement? There is no right or wrong way here - experiment and find out what works best (and then come back to answer the following).\n",
    "\n",
    "**Q**: What setup did you ultimately go with for this combined task? Explain briefly.\n",
    "\n",
    "**A**: I came up with an algorithm where the agent first masters the take off task and once it achieves the take off, it learns to hover, which finally culminates in learning of the landing task. I kept the network and the weights same so that take off, hovering, and learning could utilise a single set of weight matrices. As for each individual tasks, I have reused the takeoff, hovering, and landing subtasks which were managed by an overall task that decided when to invoke which sub task. \n",
    "\n",
    "### Implement combined agent\n",
    "\n",
    "Using your end-to-end task, implement the combined agent so that it learns to takeoff (at least 10 units above ground), hover (again, at least 10 units above ground), and gently come back to ground level.\n",
    "\n",
    "### Combination scheme and implementation notes\n",
    "\n",
    "Just like the task itself, it's up to you whether you want to train three separate (sub-)agents, or a single agent for the complete end-to-end task.\n",
    "\n",
    "**Q**: What did you end up doing? What challenges did you face, and how did you resolve them? Discuss any other implementation notes below.\n",
    "\n",
    "**A**:I have used a single agent to train all the three sub tasks. The challenges were mostly in deciding overall reward and the timing of transition from one task to another.\n",
    "A whole lot of iterations with different reward funcitons were used until a set of optimal values are found. At one point I was considering using other algorithms like DQN which works well in discrete action value space, but the issue there would have been to convert the discrete space into continuous space. \n",
    "\n",
    "### Plot episode rewards\n",
    "\n",
    "As before, plot the episode rewards, either from a single run, or averaged over multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f139da60940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Read and plot episode rewards\n",
    "plot_results('combined.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflections\n",
    "\n",
    "**Q**: Briefly summarize your experience working on this project. You can use the following prompts for ideas.\n",
    "\n",
    "- What was the hardest part of the project? (e.g. getting started, running ROS, plotting, specific task, etc.)\n",
    "- How did you approach each task and choose an appropriate algorithm/implementation for it?\n",
    "- Did you find anything interesting in how the quadcopter or your agent behaved?\n",
    "\n",
    "**A**: For me the hardest part of the project was to come up with the model for take off task. It took several days to figure out the constraints and the model required for implementing the take off task. Hovering and landing were relatively easier. The initial setup was not great deal of a hurdle for me as I could leverage other's experience about setting up the environment and the simulator as discussed on the slack channel. As for take off task, most of the time I ended up spending in understanding of the problem and the given sample code. The RL being so different from other DL models, introduced a bit of a challenge.\n",
    "I analysed the objective of each task and accordingly came up with the boundary conditions required for state and action spaces for each task so that learning becomes simpler for each task. The key here is to reduce the dimensionality of the problem space.\n",
    "Initially, when I was trying to come up with a generic algorithm to make the take off task learn in an environment with a higher degrees of freedom, I did notice that it learnt to take off gradually but only upto a point. Very soon it started to drift horizontally and eventually, after a while, instead of taking off vertically it started to fly in horizontal space steadily instead of gaining any altitude. I couldn't change this behavior by any amount of change in  the hyper parameters or reward amount. Eventually I was able to train it by constraining the displacement and velocities to z direction only. I still do not know if its possible to learn of a more generic function for take off, hover and landing tasks that can work in unconstrained 3D space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
